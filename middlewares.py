# -*- coding: utf-8 -*-# Define here the models for your spider middleware## See documentation in:# https://doc.scrapy.org/en/latest/topics/spider-middleware.htmlimport requestsclass ChangeIPMiddleware(object):    def __init__(self):        # self.proxy = 'http://'+self.get_proxy()        self.proxy='http://103.231.63.145:53281'    def get_proxy(self):        try:            newproxy = requests.get('http://192.168.31.192:5555/random')            if newproxy.status_code==200:                print('获取新的IP：'+ newproxy.text)                return newproxy.text            else:                print('接口错误'+newproxy.status_code)                return None        except requests.ConnectionError as e:                print(e.args)                return None    def process_request(self, request, spider):        proxy = self.proxy        request.meta['proxy'] = self.proxy        request.meta['download_timeout'] = 1        print(request.meta)    def process_response(self, request, response, spider):        if response:            if response.status == 200:                print('连接成功')                return response            else:                print('服务器拒绝')                print(response.status)                self.proxy = 'http://' + self.get_proxy()                return request        else:            print('不能获取response数据')            self.proxy = 'http://' + self.get_proxy()            return request    def process_exception(self, request, exception, spider):        # Called when a download handler or a process_request()        # (from other downloader middleware) raises an exception.        # Must either:        # - return None: continue processing this exception        # - return a Response object: stops process_exception() chain        # - return a Request object: stops process_exception() chain        print('连接异常')        self.proxy = 'http://' + self.get_proxy()        return request